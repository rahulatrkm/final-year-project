{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GazeCapture.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coEEkABM7Cvf",
        "colab_type": "text"
      },
      "source": [
        "### Upload the dataset for the current runtime.\n",
        "After uploading the dataset unzip using `tar`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJyRyRoegicp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl99PYXxgqun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq first\\ three.zip -C dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1buqOQgF7HGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf 00005.tar.gz -C first\\ three"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl2Obd4dnwJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For urgent codes\n",
        "!unzip -qq prepared.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRvgTveaDYEC",
        "colab_type": "text"
      },
      "source": [
        "## Prepare the data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDPnl5pcA5Ix",
        "colab_type": "code",
        "outputId": "4da1f855-a32e-42da-a966-9b0677f06f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Important\n",
        "# Upload the reference_metadata.mat file also.\n",
        "# dataset - unzip all the folders in dataset folder\n",
        "# prepared - create a folder for storing the prepared dataset\n",
        "\n",
        "import math, shutil, os, time, argparse, json, re, sys\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "'''\n",
        "Prepares the GazeCapture dataset for use with the pytorch code. Crops images, compiles JSONs into metadata.mat\n",
        "\n",
        "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018.\n",
        "\n",
        "Website: http://gazecapture.csail.mit.edu/\n",
        "\n",
        "Cite:\n",
        "\n",
        "Eye Tracking for Everyone\n",
        "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
        "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
        "\n",
        "@inproceedings{cvpr2016_gazecapture,\n",
        "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
        "Title = {Eye Tracking for Everyone},\n",
        "Year = {2016},\n",
        "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "def main():\n",
        "    # list recordings\n",
        "    recordings = os.listdir('dataset')\n",
        "    recordings = np.array(recordings, np.object)\n",
        "    recordings = recordings[[os.path.isdir(os.path.join('dataset', r)) for r in recordings]]\n",
        "    recordings.sort()\n",
        "\n",
        "    # Output structure\n",
        "    meta = {\n",
        "        'labelRecNum': [],\n",
        "        'frameIndex': [],\n",
        "        'labelDotXCam': [],\n",
        "        'labelDotYCam': [],\n",
        "        'labelFaceGrid': [],\n",
        "    }\n",
        "\n",
        "    for i,recording in enumerate(recordings):\n",
        "        print('[%d/%d] Processing recording %s (%.2f%%)' % (i, len(recordings), recording, i / len(recordings) * 100))\n",
        "        recDir = os.path.join('dataset', recording)\n",
        "        recDirOut = os.path.join('prepared', recording)\n",
        "\n",
        "        # Read JSONs\n",
        "        appleFace = readJson(os.path.join(recDir, 'appleFace.json'))\n",
        "        if appleFace is None:\n",
        "            continue\n",
        "        appleLeftEye = readJson(os.path.join(recDir, 'appleLeftEye.json'))\n",
        "        if appleLeftEye is None:\n",
        "            continue\n",
        "        appleRightEye = readJson(os.path.join(recDir, 'appleRightEye.json'))\n",
        "        if appleRightEye is None:\n",
        "            continue\n",
        "        dotInfo = readJson(os.path.join(recDir, 'dotInfo.json'))\n",
        "        if dotInfo is None:\n",
        "            continue\n",
        "        faceGrid = readJson(os.path.join(recDir, 'faceGrid.json'))\n",
        "        if faceGrid is None:\n",
        "            continue\n",
        "        frames = readJson(os.path.join(recDir, 'frames.json'))\n",
        "        if frames is None:\n",
        "            continue\n",
        "        # info = readJson(os.path.join(recDir, 'info.json'))\n",
        "        # if info is None:\n",
        "        #     continue\n",
        "        # screen = readJson(os.path.join(recDir, 'screen.json'))\n",
        "        # if screen is None:\n",
        "        #     continue\n",
        "\n",
        "        facePath = preparePath(os.path.join(recDirOut, 'appleFace'))\n",
        "        leftEyePath = preparePath(os.path.join(recDirOut, 'appleLeftEye'))\n",
        "        rightEyePath = preparePath(os.path.join(recDirOut, 'appleRightEye'))\n",
        "\n",
        "        # Preprocess\n",
        "        allValid = np.logical_and(np.logical_and(appleFace['IsValid'], appleLeftEye['IsValid']), np.logical_and(appleRightEye['IsValid'], faceGrid['IsValid']))\n",
        "        if not np.any(allValid):\n",
        "            continue\n",
        "\n",
        "        frames = np.array([int(re.match('(\\d{5})\\.jpg$', x).group(1)) for x in frames])\n",
        "\n",
        "        bboxFromJson = lambda data: np.stack((data['X'], data['Y'], data['W'],data['H']), axis=1).astype(int)\n",
        "        faceBbox = bboxFromJson(appleFace) + [-1,-1,1,1] # for compatibility with matlab code\n",
        "        leftEyeBbox = bboxFromJson(appleLeftEye) + [0,-1,0,0]\n",
        "        rightEyeBbox = bboxFromJson(appleRightEye) + [0,-1,0,0]\n",
        "        leftEyeBbox[:,:2] += faceBbox[:,:2] # relative to face\n",
        "        rightEyeBbox[:,:2] += faceBbox[:,:2]\n",
        "        faceGridBbox = bboxFromJson(faceGrid)\n",
        "\n",
        "\n",
        "        for j,frame in enumerate(frames):\n",
        "            # Can we use it?\n",
        "            if not allValid[j]:\n",
        "                continue\n",
        "\n",
        "            # Load image\n",
        "            imgFile = os.path.join(recDir, 'frames', '%05d.jpg' % frame)\n",
        "            if not os.path.isfile(imgFile):\n",
        "                logError('Warning: Could not read image file %s!' % imgFile)\n",
        "                continue\n",
        "            img = Image.open(imgFile)\n",
        "            if img is None:\n",
        "                logError('Warning: Could not read image file %s!' % imgFile)\n",
        "                continue\n",
        "            img = np.array(img.convert('RGB'))\n",
        "\n",
        "            # Crop images\n",
        "            imFace = cropImage(img, faceBbox[j,:])\n",
        "            imEyeL = cropImage(img, leftEyeBbox[j,:])\n",
        "            imEyeR = cropImage(img, rightEyeBbox[j,:])\n",
        "\n",
        "            # Save images\n",
        "            Image.fromarray(imFace).save(os.path.join(facePath, '%05d.jpg' % frame), quality=95)\n",
        "            Image.fromarray(imEyeL).save(os.path.join(leftEyePath, '%05d.jpg' % frame), quality=95)\n",
        "            Image.fromarray(imEyeR).save(os.path.join(rightEyePath, '%05d.jpg' % frame), quality=95)\n",
        "\n",
        "            # Collect metadata\n",
        "            meta['labelRecNum'] += [int(recording)]\n",
        "            meta['frameIndex'] += [frame]\n",
        "            meta['labelDotXCam'] += [dotInfo['XCam'][j]]\n",
        "            meta['labelDotYCam'] += [dotInfo['YCam'][j]]\n",
        "            meta['labelFaceGrid'] += [faceGridBbox[j,:]]\n",
        "\n",
        "\n",
        "    # Integrate\n",
        "    meta['labelRecNum'] = np.stack(meta['labelRecNum'], axis = 0).astype(np.int16)\n",
        "    meta['frameIndex'] = np.stack(meta['frameIndex'], axis = 0).astype(np.int32)\n",
        "    meta['labelDotXCam'] = np.stack(meta['labelDotXCam'], axis = 0)\n",
        "    meta['labelDotYCam'] = np.stack(meta['labelDotYCam'], axis = 0)\n",
        "    meta['labelFaceGrid'] = np.stack(meta['labelFaceGrid'], axis = 0).astype(np.uint8)\n",
        "\n",
        "    # Load reference metadata\n",
        "    print('Will compare to the reference GitHub dataset metadata.mat...')\n",
        "    reference = sio.loadmat('./reference_metadata.mat', struct_as_record=False)\n",
        "    reference['labelRecNum'] = reference['labelRecNum'].flatten()\n",
        "    reference['frameIndex'] = reference['frameIndex'].flatten()\n",
        "    reference['labelDotXCam'] = reference['labelDotXCam'].flatten()\n",
        "    reference['labelDotYCam'] = reference['labelDotYCam'].flatten()\n",
        "    reference['labelTrain'] = reference['labelTrain'].flatten()\n",
        "    reference['labelVal'] = reference['labelVal'].flatten()\n",
        "    reference['labelTest'] = reference['labelTest'].flatten()\n",
        "\n",
        "    # Find mapping\n",
        "    mKey = np.array(['%05d_%05d' % (rec, frame) for rec, frame in zip(meta['labelRecNum'], meta['frameIndex'])], np.object)\n",
        "    rKey = np.array(['%05d_%05d' % (rec, frame) for rec, frame in zip(reference['labelRecNum'], reference['frameIndex'])], np.object)\n",
        "    mIndex = {k: i for i,k in enumerate(mKey)}\n",
        "    rIndex = {k: i for i,k in enumerate(rKey)}\n",
        "    mToR = np.zeros((len(mKey,)),int) - 1\n",
        "    for i,k in enumerate(mKey):\n",
        "        if k in rIndex:\n",
        "            mToR[i] = rIndex[k]\n",
        "        else:\n",
        "            logError('Did not find rec_frame %s from the new dataset in the reference dataset!' % k)\n",
        "    rToM = np.zeros((len(rKey,)),int) - 1\n",
        "    for i,k in enumerate(rKey):\n",
        "        if k in mIndex:\n",
        "            rToM[i] = mIndex[k]\n",
        "        else:\n",
        "            logError('Did not find rec_frame %s from the reference dataset in the new dataset!' % k, critical = False)\n",
        "            #break\n",
        "\n",
        "    # Copy split from reference\n",
        "    meta['labelTrain'] = np.zeros((len(meta['labelRecNum'],)),np.bool)\n",
        "    meta['labelVal'] = np.ones((len(meta['labelRecNum'],)),np.bool) # default choice\n",
        "    meta['labelTest'] = np.zeros((len(meta['labelRecNum'],)),np.bool)\n",
        "\n",
        "    validMappingMask = mToR >= 0\n",
        "    meta['labelTrain'][validMappingMask] = reference['labelTrain'][mToR[validMappingMask]]\n",
        "    meta['labelVal'][validMappingMask] = reference['labelVal'][mToR[validMappingMask]]\n",
        "    meta['labelTest'][validMappingMask] = reference['labelTest'][mToR[validMappingMask]]\n",
        "\n",
        "    # Write out metadata\n",
        "    metaFile = os.path.join('prepared', 'metadata.mat')\n",
        "    print('Writing out the metadata.mat to %s...' % metaFile)\n",
        "    sio.savemat(metaFile, meta)\n",
        "\n",
        "    # Statistics\n",
        "    nMissing = np.sum(rToM < 0)\n",
        "    nExtra = np.sum(mToR < 0)\n",
        "    totalMatch = len(mKey) == len(rKey) and np.all(np.equal(mKey, rKey))\n",
        "    print('======================\\n\\tSummary\\n======================')\n",
        "    print('Total added %d frames from %d recordings.' % (len(meta['frameIndex']), len(np.unique(meta['labelRecNum']))))\n",
        "    if nMissing > 0:\n",
        "        print('There are %d frames missing in the new dataset. This may affect the results. Check the log to see which files are missing.' % nMissing)\n",
        "    else:\n",
        "        print('There are no missing files.')\n",
        "    if nExtra > 0:\n",
        "        print('There are %d extra frames in the new dataset. This is generally ok as they were marked for validation split only.' % nExtra)\n",
        "    else:\n",
        "        print('There are no extra files that were not in the reference dataset.')\n",
        "    if totalMatch:\n",
        "        print('The new metadata.mat is an exact match to the reference from GitHub (including ordering)')\n",
        "\n",
        "    #import pdb; pdb.set_trace()\n",
        "    input(\"Press Enter to continue...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def readJson(filename):\n",
        "    if not os.path.isfile(filename):\n",
        "        logError('Warning: No such file %s!' % filename)\n",
        "        return None\n",
        "\n",
        "    with open(filename) as f:\n",
        "        try:\n",
        "            data = json.load(f)\n",
        "        except:\n",
        "            data = None\n",
        "\n",
        "    if data is None:\n",
        "        logError('Warning: Could not read file %s!' % filename)\n",
        "        return None\n",
        "\n",
        "    return data\n",
        "\n",
        "def preparePath(path, clear = False):\n",
        "    if not os.path.isdir(path):\n",
        "        os.makedirs(path, 0o777)\n",
        "    if clear:\n",
        "        files = os.listdir(path)\n",
        "        for f in files:\n",
        "            fPath = os.path.join(path, f)\n",
        "            if os.path.isdir(fPath):\n",
        "                shutil.rmtree(fPath)\n",
        "            else:\n",
        "                os.remove(fPath)\n",
        "\n",
        "    return path\n",
        "\n",
        "def logError(msg, critical = False):\n",
        "    print(msg)\n",
        "    if critical:\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def cropImage(img, bbox):\n",
        "    bbox = np.array(bbox, int)\n",
        "\n",
        "    aSrc = np.maximum(bbox[:2], 0)\n",
        "    bSrc = np.minimum(bbox[:2] + bbox[2:], (img.shape[1], img.shape[0]))\n",
        "\n",
        "    aDst = aSrc - bbox[:2]\n",
        "    bDst = aDst + (bSrc - aSrc)\n",
        "\n",
        "    res = np.zeros((bbox[3], bbox[2], img.shape[2]), img.dtype)\n",
        "    res[aDst[1]:bDst[1],aDst[0]:bDst[0],:] = img[aSrc[1]:bSrc[1],aSrc[0]:bSrc[0],:]\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    print('DONE')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00002  00002.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LyvO9GO1Xq8",
        "colab_type": "text"
      },
      "source": [
        "## Training my own model.\n",
        "This part of the code is implemented with a little aspiration from [this project](https://colab.research.google.com/drive/1eU0fWMDtXROxY9KMgp4wLMlzc2ufMuim)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lThv63oRhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "014ea340-d50b-44a2-f8eb-a3475fd9da6c"
      },
      "source": [
        "import math, shutil, os, time, argparse\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "\n",
        "from ITrackerData import ITrackerData\n",
        "from ITrackerModel import ITrackerModel\n",
        "\n",
        "'''\n",
        "Train/test code for iTracker.\n",
        "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018. \n",
        "Website: http://gazecapture.csail.mit.edu/\n",
        "Cite:\n",
        "Eye Tracking for Everyone\n",
        "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
        "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
        "@inproceedings{cvpr2016_gazecapture,\n",
        "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
        "Title = {Eye Tracking for Everyone},\n",
        "Year = {2016},\n",
        "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
        "}\n",
        "'''\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "parser = argparse.ArgumentParser(description='iTracker-pytorch-Trainer.')\n",
        "parser.add_argument('--data_path', help=\"Path to processed dataset. It should contain metadata.mat. Use prepareDataset.py.\")\n",
        "parser.add_argument('--sink', type=str2bool, nargs='?', const=True, default=False, help=\"Just sink and terminate.\")\n",
        "parser.add_argument('--reset', type=str2bool, nargs='?', const=True, default=False, help=\"Start from scratch (do not load).\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Change there flags to control what happens.\n",
        "doLoad = not args.reset # Load checkpoint at the beginning\n",
        "doTest = args.sink # Only run test, no training\n",
        "\n",
        "workers = 16\n",
        "epochs = 25\n",
        "batch_size = torch.cuda.device_count()*100 # Change if out of cuda memory\n",
        "\n",
        "base_lr = 0.0001\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4\n",
        "print_freq = 10\n",
        "prec1 = 0\n",
        "best_prec1 = 1e20\n",
        "lr = base_lr\n",
        "\n",
        "count_test = 0\n",
        "count = 0\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    global args, best_prec1, weight_decay, momentum\n",
        "\n",
        "    model = ITrackerModel()\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    model.cuda()\n",
        "    imSize=(224,224)\n",
        "    cudnn.benchmark = True   \n",
        "\n",
        "    epoch = 0\n",
        "    if doLoad:\n",
        "        saved = load_checkpoint()\n",
        "        if saved:\n",
        "            print('Loading checkpoint for epoch %05d with loss %.5f (which is the mean squared error not the actual linear error)...' % (saved['epoch'], saved['best_prec1']))\n",
        "            state = saved['state_dict']\n",
        "            try:\n",
        "                model.module.load_state_dict(state)\n",
        "            except:\n",
        "                model.load_state_dict(state)\n",
        "            epoch = saved['epoch']\n",
        "            best_prec1 = saved['best_prec1']\n",
        "        else:\n",
        "            print('Warning: Could not read checkpoint!')\n",
        "\n",
        "    \n",
        "    dataTrain = ITrackerData(dataPath = args.data_path, split='train', imSize = imSize)\n",
        "    dataVal = ITrackerData(dataPath = args.data_path, split='test', imSize = imSize)\n",
        "   \n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        dataTrain,\n",
        "        batch_size=batch_size, shuffle=True,\n",
        "        num_workers=workers, pin_memory=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        dataVal,\n",
        "        batch_size=batch_size, shuffle=False,\n",
        "        num_workers=workers, pin_memory=True)\n",
        "\n",
        "\n",
        "    criterion = nn.MSELoss().cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr,\n",
        "                                momentum=momentum,\n",
        "                                weight_decay=weight_decay)\n",
        "\n",
        "    # Quick test\n",
        "    if doTest:\n",
        "        validate(val_loader, model, criterion, epoch)\n",
        "        return\n",
        "\n",
        "    for epoch in range(0, epoch):\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "        \n",
        "    for epoch in range(epoch, epochs):\n",
        "        adjust_learning_rate(optimizer, epoch)\n",
        "\n",
        "        # train for one epoch\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion, epoch)\n",
        "\n",
        "        # remember best prec@1 and save checkpoint\n",
        "        is_best = prec1 < best_prec1\n",
        "        best_prec1 = min(prec1, best_prec1)\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "        }, is_best)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion,optimizer, epoch):\n",
        "    global count\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(train_loader):\n",
        "        \n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        imFace = imFace.cuda(async=True)\n",
        "        imEyeL = imEyeL.cuda(async=True)\n",
        "        imEyeR = imEyeR.cuda(async=True)\n",
        "        faceGrid = faceGrid.cuda(async=True)\n",
        "        gaze = gaze.cuda(async=True)\n",
        "        \n",
        "        imFace = torch.autograd.Variable(imFace, requires_grad = True)\n",
        "        imEyeL = torch.autograd.Variable(imEyeL, requires_grad = True)\n",
        "        imEyeR = torch.autograd.Variable(imEyeR, requires_grad = True)\n",
        "        faceGrid = torch.autograd.Variable(faceGrid, requires_grad = True)\n",
        "        gaze = torch.autograd.Variable(gaze, requires_grad = False)\n",
        "\n",
        "        # compute output\n",
        "        output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
        "\n",
        "        loss = criterion(output, gaze)\n",
        "        \n",
        "        losses.update(loss.data.item(), imFace.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        count=count+1\n",
        "\n",
        "        print('Epoch (train): [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
        "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses))\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    global count_test\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    lossesLin = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    end = time.time()\n",
        "\n",
        "\n",
        "    oIndex = 0\n",
        "    for i, (row, imFace, imEyeL, imEyeR, faceGrid, gaze) in enumerate(val_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        imFace = imFace.cuda(async=True)\n",
        "        imEyeL = imEyeL.cuda(async=True)\n",
        "        imEyeR = imEyeR.cuda(async=True)\n",
        "        faceGrid = faceGrid.cuda(async=True)\n",
        "        gaze = gaze.cuda(async=True)\n",
        "        \n",
        "        imFace = torch.autograd.Variable(imFace, requires_grad = False)\n",
        "        imEyeL = torch.autograd.Variable(imEyeL, requires_grad = False)\n",
        "        imEyeR = torch.autograd.Variable(imEyeR, requires_grad = False)\n",
        "        faceGrid = torch.autograd.Variable(faceGrid, requires_grad = False)\n",
        "        gaze = torch.autograd.Variable(gaze, requires_grad = False)\n",
        "\n",
        "        # compute output\n",
        "        with torch.no_grad():\n",
        "            output = model(imFace, imEyeL, imEyeR, faceGrid)\n",
        "\n",
        "        loss = criterion(output, gaze)\n",
        "        \n",
        "        lossLin = output - gaze\n",
        "        lossLin = torch.mul(lossLin,lossLin)\n",
        "        lossLin = torch.sum(lossLin,1)\n",
        "        lossLin = torch.mean(torch.sqrt(lossLin))\n",
        "\n",
        "        losses.update(loss.data.item(), imFace.size(0))\n",
        "        lossesLin.update(lossLin.item(), imFace.size(0))\n",
        "     \n",
        "        # compute gradient and do SGD step\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "\n",
        "        print('Epoch (val): [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Error L2 {lossLin.val:.4f} ({lossLin.avg:.4f})\\t'.format(\n",
        "                    epoch, i, len(val_loader), batch_time=batch_time,\n",
        "                   loss=losses,lossLin=lossesLin))\n",
        "\n",
        "    return lossesLin.avg\n",
        "\n",
        "CHECKPOINTS_PATH = '.'\n",
        "\n",
        "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
        "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
        "    print(filename)\n",
        "    if not os.path.isfile(filename):\n",
        "        return None\n",
        "    state = torch.load(filename)\n",
        "    return state\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    if not os.path.isdir(CHECKPOINTS_PATH):\n",
        "        os.makedirs(CHECKPOINTS_PATH, 0o777)\n",
        "    bestFilename = os.path.join(CHECKPOINTS_PATH, 'best_' + filename)\n",
        "    filename = os.path.join(CHECKPOINTS_PATH, filename)\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, bestFilename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = base_lr * (0.1 ** (epoch // 30))\n",
        "    for param_group in optimizer.state_dict()['param_groups']:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    print('DONE')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9749e0a8b790>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mITrackerData\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mITrackerData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mITrackerModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mITrackerModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ITrackerData'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8laQpyNsbba",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0b2db1aa-ced0-471d-8646-8fdbbc5fd9ec"
      },
      "source": [
        "## Upload the main.py file\n",
        "## Also the other itracker files\n",
        "!python main.py --data_path 'prepared' --reset"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading iTracker dataset...\n",
            "\tReading metadata from prepared/metadata.mat...\n",
            "\tReading metadata from ./mean_face_224.mat...\n",
            "\tReading metadata from ./mean_left_224.mat...\n",
            "\tReading metadata from ./mean_right_224.mat...\n",
            "Loaded iTracker dataset split \"train\" with 159 records...\n",
            "Loading iTracker dataset...\n",
            "\tReading metadata from prepared/metadata.mat...\n",
            "\tReading metadata from ./mean_face_224.mat...\n",
            "\tReading metadata from ./mean_left_224.mat...\n",
            "\tReading metadata from ./mean_right_224.mat...\n",
            "Loaded iTracker dataset split \"test\" with 0 records...\n",
            "Epoch (train): [0][0/2]\tTime 9.608 (9.608)\tData 1.917 (1.917)\tLoss 23.6582 (23.6582)\t\n",
            "Epoch (train): [0][1/2]\tTime 4.576 (7.092)\tData 0.000 (0.958)\tLoss 19.3100 (22.0447)\t\n",
            "Epoch (train): [1][0/2]\tTime 8.043 (8.043)\tData 1.761 (1.761)\tLoss 22.1445 (22.1445)\t\n",
            "Epoch (train): [1][1/2]\tTime 3.779 (5.911)\tData 0.000 (0.881)\tLoss 21.8589 (22.0385)\t\n",
            "Epoch (train): [2][0/2]\tTime 8.093 (8.093)\tData 1.778 (1.778)\tLoss 21.9033 (21.9033)\t\n",
            "Epoch (train): [2][1/2]\tTime 3.733 (5.913)\tData 0.000 (0.889)\tLoss 22.2317 (22.0252)\t\n",
            "Epoch (train): [3][0/2]\tTime 8.030 (8.030)\tData 1.789 (1.789)\tLoss 22.0999 (22.0999)\t\n",
            "Epoch (train): [3][1/2]\tTime 3.783 (5.906)\tData 0.000 (0.894)\tLoss 21.8553 (22.0091)\t\n",
            "Epoch (train): [4][0/2]\tTime 8.169 (8.169)\tData 1.767 (1.767)\tLoss 20.3059 (20.3059)\t\n",
            "Epoch (train): [4][1/2]\tTime 3.837 (6.003)\tData 0.000 (0.884)\tLoss 24.8388 (21.9879)\t\n",
            "Epoch (train): [5][0/2]\tTime 8.009 (8.009)\tData 1.777 (1.777)\tLoss 22.6835 (22.6835)\t\n",
            "Epoch (train): [5][1/2]\tTime 3.739 (5.874)\tData 0.000 (0.888)\tLoss 20.7469 (21.9649)\t\n",
            "Epoch (train): [6][0/2]\tTime 8.130 (8.130)\tData 1.733 (1.733)\tLoss 20.5322 (20.5322)\t\n",
            "Epoch (train): [6][1/2]\tTime 3.793 (5.962)\tData 0.000 (0.866)\tLoss 24.3208 (21.9380)\t\n",
            "Epoch (train): [7][0/2]\tTime 8.065 (8.065)\tData 1.771 (1.771)\tLoss 21.0803 (21.0803)\t\n",
            "Epoch (train): [7][1/2]\tTime 3.790 (5.928)\tData 0.000 (0.886)\tLoss 23.3178 (21.9106)\t\n",
            "Epoch (train): [8][0/2]\tTime 8.132 (8.132)\tData 1.765 (1.765)\tLoss 22.9585 (22.9585)\t\n",
            "Epoch (train): [8][1/2]\tTime 3.826 (5.979)\tData 0.000 (0.882)\tLoss 20.0612 (21.8834)\t\n",
            "Epoch (train): [9][0/2]\tTime 8.132 (8.132)\tData 1.773 (1.773)\tLoss 22.1267 (22.1267)\t\n",
            "Epoch (train): [9][1/2]\tTime 3.866 (5.999)\tData 0.000 (0.887)\tLoss 21.3833 (21.8509)\t\n",
            "Epoch (train): [10][0/2]\tTime 8.086 (8.086)\tData 1.790 (1.790)\tLoss 22.8104 (22.8104)\t\n",
            "Epoch (train): [10][1/2]\tTime 3.829 (5.957)\tData 0.000 (0.895)\tLoss 20.1436 (21.8208)\t\n",
            "Epoch (train): [11][0/2]\tTime 8.138 (8.138)\tData 1.743 (1.743)\tLoss 21.0415 (21.0415)\t\n",
            "Epoch (train): [11][1/2]\tTime 3.741 (5.939)\tData 0.000 (0.871)\tLoss 23.0524 (21.7877)\t\n",
            "Epoch (train): [12][0/2]\tTime 8.184 (8.184)\tData 1.826 (1.826)\tLoss 21.4834 (21.4834)\t\n",
            "Epoch (train): [12][1/2]\tTime 3.849 (6.017)\tData 0.000 (0.913)\tLoss 22.2192 (21.7564)\t\n",
            "Epoch (train): [13][0/2]\tTime 8.025 (8.025)\tData 1.751 (1.751)\tLoss 22.8728 (22.8728)\t\n",
            "Epoch (train): [13][1/2]\tTime 3.891 (5.958)\tData 0.000 (0.876)\tLoss 19.7742 (21.7230)\t\n",
            "Epoch (train): [14][0/2]\tTime 8.183 (8.183)\tData 1.762 (1.762)\tLoss 21.0856 (21.0856)\t\n",
            "Epoch (train): [14][1/2]\tTime 3.840 (6.012)\tData 0.000 (0.881)\tLoss 22.7134 (21.6896)\t\n",
            "Epoch (train): [15][0/2]\tTime 8.093 (8.093)\tData 1.773 (1.773)\tLoss 21.7040 (21.7040)\t\n",
            "Epoch (train): [15][1/2]\tTime 3.777 (5.935)\tData 0.000 (0.887)\tLoss 21.5748 (21.6560)\t\n",
            "Epoch (train): [16][0/2]\tTime 8.101 (8.101)\tData 1.797 (1.797)\tLoss 22.2189 (22.2189)\t\n",
            "Epoch (train): [16][1/2]\tTime 3.791 (5.946)\tData 0.000 (0.899)\tLoss 20.6099 (21.6219)\t\n",
            "Epoch (train): [17][0/2]\tTime 8.081 (8.081)\tData 1.770 (1.770)\tLoss 21.4342 (21.4342)\t\n",
            "Epoch (train): [17][1/2]\tTime 3.716 (5.899)\tData 0.000 (0.885)\tLoss 21.8486 (21.5880)\t\n",
            "Epoch (train): [18][0/2]\tTime 8.004 (8.004)\tData 1.787 (1.787)\tLoss 20.6333 (20.6333)\t\n",
            "Epoch (train): [18][1/2]\tTime 3.683 (5.844)\tData 0.000 (0.893)\tLoss 23.1113 (21.5528)\t\n",
            "Epoch (train): [19][0/2]\tTime 8.073 (8.073)\tData 1.759 (1.759)\tLoss 20.5924 (20.5924)\t\n",
            "Epoch (train): [19][1/2]\tTime 3.775 (5.924)\tData 0.000 (0.880)\tLoss 23.0909 (21.5195)\t\n",
            "Epoch (train): [20][0/2]\tTime 7.920 (7.920)\tData 1.734 (1.734)\tLoss 21.6971 (21.6971)\t\n",
            "Epoch (train): [20][1/2]\tTime 3.731 (5.825)\tData 0.000 (0.867)\tLoss 21.1237 (21.4843)\t\n",
            "Epoch (train): [21][0/2]\tTime 7.792 (7.792)\tData 1.718 (1.718)\tLoss 21.7595 (21.7595)\t\n",
            "Epoch (train): [21][1/2]\tTime 3.651 (5.722)\tData 0.000 (0.859)\tLoss 20.9242 (21.4495)\t\n",
            "Epoch (train): [22][0/2]\tTime 8.134 (8.134)\tData 1.770 (1.770)\tLoss 22.2725 (22.2725)\t\n",
            "Epoch (train): [22][1/2]\tTime 3.719 (5.927)\tData 0.000 (0.885)\tLoss 19.9642 (21.4160)\t\n",
            "Epoch (train): [23][0/2]\tTime 8.314 (8.314)\tData 1.797 (1.797)\tLoss 21.7420 (21.7420)\t\n",
            "Epoch (train): [23][1/2]\tTime 3.880 (6.097)\tData 0.000 (0.899)\tLoss 20.7642 (21.3792)\t\n",
            "Epoch (train): [24][0/2]\tTime 8.113 (8.113)\tData 1.757 (1.757)\tLoss 21.8719 (21.8719)\t\n",
            "Epoch (train): [24][1/2]\tTime 3.823 (5.968)\tData 0.000 (0.879)\tLoss 20.4544 (21.3459)\t\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MLQgp4-vDY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "fa252a7d-d0f1-4a44-c7e3-9b00c81dbc76"
      },
      "source": [
        "## Testing the dataset\n",
        "!python main.py --data_path 'prepared' --sink"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./checkpoint.pth.tar\n",
            "Loading checkpoint for epoch 00025 with loss 0.00000 (which is the mean squared error not the actual linear error)...\n",
            "Loading iTracker dataset...\n",
            "\tReading metadata from prepared/metadata.mat...\n",
            "\tReading metadata from ./mean_face_224.mat...\n",
            "\tReading metadata from ./mean_left_224.mat...\n",
            "\tReading metadata from ./mean_right_224.mat...\n",
            "Loaded iTracker dataset split \"train\" with 159 records...\n",
            "Loading iTracker dataset...\n",
            "\tReading metadata from prepared/metadata.mat...\n",
            "\tReading metadata from ./mean_face_224.mat...\n",
            "\tReading metadata from ./mean_left_224.mat...\n",
            "\tReading metadata from ./mean_right_224.mat...\n",
            "Loaded iTracker dataset split \"test\" with 0 records...\n",
            "DONE\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}